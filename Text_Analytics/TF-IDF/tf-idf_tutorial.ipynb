{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Using-TextBlob\" data-toc-modified-id=\"Using-TextBlob-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Using TextBlob</a></span></li><li><span><a href=\"#SKLearn-TF-IDF-Vectorizer\" data-toc-modified-id=\"SKLearn-TF-IDF-Vectorizer-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>SKLearn TF-IDF Vectorizer</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dict-to-List\" data-toc-modified-id=\"Dict-to-List-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Dict to List</a></span></li><li><span><a href=\"#Form-TF-IDF-Matrix\" data-toc-modified-id=\"Form-TF-IDF-Matrix-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Form TF-IDF Matrix</a></span></li><li><span><a href=\"#Final\" data-toc-modified-id=\"Final-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Final</a></span></li></ul></li><li><span><a href=\"#Sklearn-Advanced-Method\" data-toc-modified-id=\"Sklearn-Advanced-Method-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Sklearn Advanced Method</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tokenize-Function\" data-toc-modified-id=\"Tokenize-Function-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Tokenize Function</a></span></li><li><span><a href=\"#Get_Vocab-Function\" data-toc-modified-id=\"Get_Vocab-Function-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Get_Vocab Function</a></span></li><li><span><a href=\"#Build-Model-Using-Custom-Parameters\" data-toc-modified-id=\"Build-Model-Using-Custom-Parameters-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Build Model Using Custom Parameters</a></span></li><li><span><a href=\"#MAX_DF\" data-toc-modified-id=\"MAX_DF-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>MAX_DF</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TextBlob\n",
    "\n",
    "Credit: https://stevenloria.com/tf-idf/\n",
    "\n",
    "I don't think I will use this approach but it's a very succint implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from textblob import TextBlob as tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf(word, blob):\n",
    "    return blob.words.count(word) / len(blob.words)\n",
    "\n",
    "def n_containing(word, bloblist):\n",
    "    return sum(1 for blob in bloblist if word in blob.words)\n",
    "\n",
    "def idf(word, bloblist):\n",
    "    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))\n",
    "\n",
    "def tfidf(word, blob, bloblist):\n",
    "    return tf(word, blob) * idf(word, bloblist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in document 1\n",
      "\tWord: python, TF-IDF: 0.01662\n",
      "\tWord: films, TF-IDF: 0.00997\n",
      "\tWord: made-for-TV, TF-IDF: 0.00665\n",
      "Top words in document 2\n",
      "\tWord: genus, TF-IDF: 0.02192\n",
      "\tWord: 2, TF-IDF: 0.02192\n",
      "\tWord: from, TF-IDF: 0.01096\n",
      "Top words in document 3\n",
      "\tWord: Colt, TF-IDF: 0.01367\n",
      "\tWord: Magnum, TF-IDF: 0.01367\n",
      "\tWord: revolver, TF-IDF: 0.01367\n"
     ]
    }
   ],
   "source": [
    "document1 = tb(\"\"\"Python is a 2000 made-for-TV horror movie directed by Richard\n",
    "Clabaugh. The film features several cult favorite actors, including William\n",
    "Zabka of The Karate Kid fame, Wil Wheaton, Casper Van Dien, Jenny McCarthy,\n",
    "Keith Coogan, Robert Englund (best known for his role as Freddy Krueger in the\n",
    "A Nightmare on Elm Street series of films), Dana Barron, David Bowe, and Sean\n",
    "Whalen. The film concerns a genetically engineered snake, a python, that\n",
    "escapes and unleashes itself on a small town. It includes the classic final\n",
    "girl scenario evident in films like Friday the 13th. It was filmed in Los Angeles,\n",
    " California and Malibu, California. Python was followed by two sequels: Python\n",
    " II (2002) and Boa vs. Python (2004), both also made-for-TV films.\"\"\")\n",
    "\n",
    "document2 = tb(\"\"\"Python, from the Greek word (πύθων/πύθωνας), is a genus of\n",
    "nonvenomous pythons[2] found in Africa and Asia. Currently, 7 species are\n",
    "recognised.[2] A member of this genus, P. reticulatus, is among the longest\n",
    "snakes known.\"\"\")\n",
    "\n",
    "document3 = tb(\"\"\"The Colt Python is a .357 Magnum caliber revolver formerly\n",
    "manufactured by Colt's Manufacturing Company of Hartford, Connecticut.\n",
    "It is sometimes referred to as a \"Combat Magnum\".[1] It was first introduced\n",
    "in 1955, the same year as Smith &amp; Wesson's M29 .44 Magnum. The now discontinued\n",
    "Colt Python targeted the premium revolver market segment. Some firearm\n",
    "collectors and writers such as Jeff Cooper, Ian V. Hogg, Chuck Hawks, Leroy\n",
    "Thompson, Renee Smeets and Martin Dougherty have described the Python as the\n",
    "finest production revolver ever made.\"\"\")\n",
    "\n",
    "bloblist = [document1, document2, document3]\n",
    "\n",
    "for i, blob in enumerate(bloblist):\n",
    "    print(\"Top words in document {}\".format(i + 1))\n",
    "    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, score in sorted_words[:3]:\n",
    "        print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in document 1\n",
      "\tWord: the, TF-IDF: 0.0901\n",
      "\tWord: quick, TF-IDF: 0.04505\n",
      "\tWord: brown, TF-IDF: 0.04505\n",
      "Top words in document 2\n",
      "\tWord: dog, TF-IDF: 0.0\n",
      "\tWord: The, TF-IDF: -0.14384\n",
      "Top words in document 3\n",
      "\tWord: fox, TF-IDF: 0.0\n",
      "\tWord: The, TF-IDF: -0.14384\n"
     ]
    }
   ],
   "source": [
    "# comparing with the second method \n",
    "\n",
    "bloblist = [tb(\"The quick brown fox jumped over the lazy dog.\"),\n",
    "            tb(\"The dog.\"),\n",
    "            tb(\"The fox\")]\n",
    "\n",
    "for i, blob in enumerate(bloblist):\n",
    "    print(\"Top words in document {}\".format(i + 1))\n",
    "    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, score in sorted_words[:3]:\n",
    "        print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLearn TF-IDF Vectorizer\n",
    "\n",
    "Credit: https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/\n",
    "\n",
    "In the end, I want to find out top N words from each document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "        \"The dog.\",\n",
    "        \"The fox\"]\n",
    "\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "[ 1.69314718  1.28768207  1.28768207  1.69314718  1.69314718  1.69314718\n",
      "  1.69314718  1.        ]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n",
      "\n",
      "[[ 0.36388646  0.27674503  0.27674503  0.36388646  0.36388646  0.36388646\n",
      "   0.36388646  0.42983441]]\n"
     ]
    }
   ],
   "source": [
    "# encode document\n",
    "vector = vectorizer.transform([text[0]])\n",
    "\n",
    "print(vector.shape)\n",
    "print()\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.78980693,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.61335554]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform([text[1]]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectors = vectorizer.transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# row = text, col = each vocab \n",
    "vectors.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.78980693,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.61335554]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# corresponds to 'the dog', and the rest vocabs are 0  \n",
    "vectors[1].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dict to List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brown': 0,\n",
       " 'dog': 1,\n",
       " 'fox': 2,\n",
       " 'jumped': 3,\n",
       " 'lazy': 4,\n",
       " 'over': 5,\n",
       " 'quick': 6,\n",
       " 'the': 7}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of vocabs in the tf-idf \n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brown', 'dog', 'fox', 'jumped', 'lazy', 'over', 'quick', 'the']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the vocab dict to list in the order of its value \n",
    "vocab= sorted(vectorizer.vocabulary_, key=vectorizer.vocabulary_.get, reverse=False)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.36388646,  0.27674503,  0.27674503,  0.36388646,  0.36388646,\n",
       "        0.36388646,  0.36388646,  0.42983441])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors[0].toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36388645548 brown\n",
      "0.276745028731 dog\n",
      "0.276745028731 fox\n",
      "0.36388645548 jumped\n",
      "0.36388645548 lazy\n",
      "0.36388645548 over\n",
      "0.36388645548 quick\n",
      "0.429834405016 the\n"
     ]
    }
   ],
   "source": [
    "for i,s in zip(vectors[0].toarray()[0],vocab):\n",
    "    print(i,s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Form TF-IDF Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brown</th>\n",
       "      <th>dog</th>\n",
       "      <th>fox</th>\n",
       "      <th>jumped</th>\n",
       "      <th>lazy</th>\n",
       "      <th>over</th>\n",
       "      <th>quick</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.363886</td>\n",
       "      <td>0.276745</td>\n",
       "      <td>0.276745</td>\n",
       "      <td>0.363886</td>\n",
       "      <td>0.363886</td>\n",
       "      <td>0.363886</td>\n",
       "      <td>0.363886</td>\n",
       "      <td>0.429834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.789807</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.613356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.789807</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.613356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      brown       dog       fox    jumped      lazy      over     quick  \\\n",
       "0  0.363886  0.276745  0.276745  0.363886  0.363886  0.363886  0.363886   \n",
       "1  0.000000  0.789807  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.789807  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "        the  \n",
       "0  0.429834  \n",
       "1  0.613356  \n",
       "2  0.613356  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(vectors.toarray(),columns=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.42983440501598907, 'the'),\n",
       " (0.36388645548024179, 'brown'),\n",
       " (0.36388645548024179, 'jumped'),\n",
       " (0.36388645548024179, 'lazy'),\n",
       " (0.36388645548024179, 'over'),\n",
       " (0.36388645548024179, 'quick'),\n",
       " (0.27674502873103346, 'dog'),\n",
       " (0.27674502873103346, 'fox')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zipped = zip(vectors[0].toarray()[0],vocab)\n",
    "\n",
    "sorted(zipped,key = lambda t: t[0],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tWord: the, TF-IDF: 0.42983\n",
      "\tWord: brown, TF-IDF: 0.36389\n",
      "\tWord: jumped, TF-IDF: 0.36389\n"
     ]
    }
   ],
   "source": [
    "zipped = zip(vocab,vectors[0].toarray()[0])\n",
    "sorted_words = sorted(zipped,key = lambda t: t[1],reverse=True)\n",
    "for word, score in sorted_words[:3]:\n",
    "        print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 0.42983440501598907),\n",
       " ('brown', 0.36388645548024179),\n",
       " ('jumped', 0.36388645548024179)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_words[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in document 1\n",
      "\tWord: the, TF-IDF: 0.42983\n",
      "\tWord: brown, TF-IDF: 0.36389\n",
      "\tWord: jumped, TF-IDF: 0.36389\n",
      "Top words in document 2\n",
      "\tWord: dog, TF-IDF: 0.78981\n",
      "\tWord: the, TF-IDF: 0.61336\n",
      "\tWord: brown, TF-IDF: 0.0\n",
      "Top words in document 3\n",
      "\tWord: fox, TF-IDF: 0.78981\n",
      "\tWord: the, TF-IDF: 0.61336\n",
      "\tWord: brown, TF-IDF: 0.0\n"
     ]
    }
   ],
   "source": [
    "for i, blob in enumerate(text):\n",
    "    print(\"Top words in document {}\".format(i + 1))\n",
    "        \n",
    "    zipped = zip(vocab,vectors[i].tsoarray()[0])\n",
    "    sorted_words = sorted(zipped,key = lambda t: t[1],reverse=True)\n",
    "\n",
    "    for word, score in sorted_words[:3]:\n",
    "            print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Advanced Method \n",
    "\n",
    "Making use of some parameters, custom tokenizer, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample', 'text', 'yo']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = 'sample text yo'\n",
    "\n",
    "def tokenize(text):\n",
    "    words = text.replace('/','.').split()\n",
    "    words = [w.strip() for w in words if w.strip()]\n",
    "    return words \n",
    "\n",
    "tokenize(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get_Vocab Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'The': 0,\n",
       " 'brown': 6,\n",
       " 'dog.': 8,\n",
       " 'fox': 2,\n",
       " 'jumped': 4,\n",
       " 'lazy': 7,\n",
       " 'over': 3,\n",
       " 'quick': 1,\n",
       " 'the': 5}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_vocab(df,tokenize_fn):\n",
    "    # build the vocabulary set\n",
    "    vocabulary = set()\n",
    "    \n",
    "    for i in df:\n",
    "        words = tokenize_fn(i)\n",
    "        vocabulary.update(words)\n",
    "\n",
    "    vocabulary= list(vocabulary)\n",
    "\n",
    "    word_index = {w: idx for idx, w in enumerate(vocabulary)}\n",
    "\n",
    "    return word_index\n",
    "\n",
    "word_index = get_vocab(text,tokenize_fn=tokenize)\n",
    "word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model Using Custom Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=tokenize, vocabulary=word_index)\n",
    "tfidf.fit(text)\n",
    "X = tfidf.transform(text)\n",
    "vocab = sorted(tfidf.vocabulary_, key=tfidf.vocabulary_.get, reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in document 1\n",
      "\tWord: the, TF-IDF: 0.42983\n",
      "\tWord: brown, TF-IDF: 0.36389\n",
      "\tWord: jumped, TF-IDF: 0.36389\n",
      "Top words in document 2\n",
      "\tWord: dog, TF-IDF: 0.78981\n",
      "\tWord: the, TF-IDF: 0.61336\n",
      "\tWord: brown, TF-IDF: 0.0\n",
      "Top words in document 3\n",
      "\tWord: fox, TF-IDF: 0.78981\n",
      "\tWord: the, TF-IDF: 0.61336\n",
      "\tWord: brown, TF-IDF: 0.0\n"
     ]
    }
   ],
   "source": [
    "for i, blob in enumerate(text):\n",
    "    print(\"Top words in document {}\".format(i + 1))\n",
    "        \n",
    "    zipped = zip(vocab,X[i].toarray()[0])\n",
    "    sorted_words = sorted(zipped,key = lambda t: t[1],reverse=True)\n",
    "\n",
    "    for word, score in sorted_words[:3]:\n",
    "            print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### MAX_DF\n",
    "\n",
    "https://stackoverflow.com/questions/27697766/understanding-min-df-and-max-df-in-scikit-countvectorizer?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa\n",
    "\n",
    "\n",
    "max_df is used for removing terms that appear too frequently, also known as \"corpus-specific stop words\". \n",
    "\n",
    "For example:\n",
    "\n",
    "- max_df = 0.50 means \"ignore terms that appear in more than 50% of the documents\".\n",
    "- max_df = 25 means \"ignore terms that appear in more than 25 documents\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
