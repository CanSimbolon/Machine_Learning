{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark Tutorial \n",
    "\n",
    "Source: \n",
    "\n",
    "- https://www.dezyre.com/apache-spark-tutorial/pyspark-tutorial\n",
    "- http://www.kdnuggets.com/2015/11/introduction-spark-python.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() \n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc =SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking if sc is working \n",
    "data = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading a file in PySpark Shell\n",
    "# you can choose anything you want. \n",
    "RDDread = sc.textFile (\"/Users/ijung/Desktop/confusion.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Apache Spark',\n",
       " '',\n",
       " 'Spark is a fast and general cluster computing system for Big Data. It provides',\n",
       " 'high-level APIs in Scala, Java, Python, and R, and an optimized engine that',\n",
       " 'supports general computation graphs for data analysis. It also supports a',\n",
       " 'rich set of higher-level tools including Spark SQL for SQL and DataFrames,',\n",
       " 'MLlib for machine learning, GraphX for graph processing,',\n",
       " 'and Spark Streaming for stream processing.',\n",
       " '',\n",
       " '<http://spark.apache.org/>',\n",
       " '',\n",
       " '## Online Documentation',\n",
       " '',\n",
       " 'You can find the latest Spark documentation, including a programming',\n",
       " 'guide, on the [project web page](http://spark.apache.org/documentation.html)',\n",
       " '',\n",
       " '',\n",
       " '## Python Packaging',\n",
       " '',\n",
       " 'This README file only contains basic information related to pip installed PySpark.',\n",
       " 'This packaging is currently experimental and may change in future versions (although we will do our best to keep compatibility).',\n",
       " 'Using PySpark requires the Spark JARs, and if you are building this from source please see the builder instructions at',\n",
       " '[\"Building Spark\"](http://spark.apache.org/docs/latest/building-spark.html).',\n",
       " '',\n",
       " 'The Python packaging for Spark is not intended to replace all of the other use cases. This Python packaged version of Spark is suitable for interacting with an existing cluster (be it Spark standalone, YARN, or Mesos) - but does not contain the tools required to setup your own standalone Spark cluster. You can download the full version of Spark from the [Apache Spark downloads page](http://spark.apache.org/downloads.html).',\n",
       " '',\n",
       " '',\n",
       " '**NOTE:** If you are using this with a Spark standalone cluster you must ensure that the version (including minor version) matches or you may experience odd errors.',\n",
       " '',\n",
       " '## Python Requirements',\n",
       " '',\n",
       " 'At its core PySpark depends on Py4J (currently version 0.10.4), but additional sub-packages have their own requirements (including numpy and pandas).']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDDread.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Apache Spark'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First () – This will return the first element from the dataset.        \n",
    "RDDread.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Apache Spark',\n",
       " '',\n",
       " 'Spark is a fast and general cluster computing system for Big Data. It provides']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take (n) - This will return the first n lines from the dataset and display them on the console.\n",
    "RDDread.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '',\n",
       " '',\n",
       " 'At its core PySpark depends on Py4J (currently version 0.10.4), but additional sub-packages have their own requirements (including numpy and pandas).',\n",
       " '## Python Requirements',\n",
       " 'This packaging is currently experimental and may change in future versions (although we will do our best to keep compatibility).',\n",
       " '# Apache Spark',\n",
       " 'supports general computation graphs for data analysis. It also supports a',\n",
       " '',\n",
       " 'and Spark Streaming for stream processing.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TakeSample (withReplacement, n, [seed]) - This action will return n elements from the dataset, with or without replacement (true or false). Seed is an optional parameter that is used as a random generator.\n",
    "RDDread.takeSample(False,10,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count () – To know the number of lines in a RDD\n",
    "RDDread.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation and Actions in Apache Spark\n",
    "\n",
    "Spark Transformations:\n",
    "- map()\n",
    "- flatMap()\n",
    "- filter()\n",
    "- sample()\n",
    "- union()\n",
    "- intersection()\n",
    "- distinct()\n",
    "- join()\n",
    "\n",
    "Spark Actions:\n",
    "- reduce()             \n",
    "- collect()               \n",
    "- count()\n",
    "- first()    \n",
    "- takeSample(withReplacement, num, [seed])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Confusion is the inability to think as clearly or quickly as you normally do.',\n",
       " '',\n",
       " 'You may  have difficulty paying attention to anything , remembering anyone, and making decisions.',\n",
       " '',\n",
       " 'Confusion may come to anyone early or late phase of the life, depending on the reason behind it .']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDDread.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Confusion',\n",
       "  'is',\n",
       "  'the',\n",
       "  'inability',\n",
       "  'to',\n",
       "  'think',\n",
       "  'as',\n",
       "  'clearly',\n",
       "  'or',\n",
       "  'quickly',\n",
       "  'as',\n",
       "  'you',\n",
       "  'normally',\n",
       "  'do.'],\n",
       " ['']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iterable of iterables\n",
    "mappedconfusion = RDDread.map(lambda line : line.split(\" \"))\n",
    "mappedconfusion.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Confusion', 'is']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  iterable of strings.\n",
    "flatMappedConfusion = RDDread.flatMap(lambda line : line.split(\" \"))\n",
    "flatMappedConfusion.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try to find out the lines having confusion term in it in the confusedRDD-\n",
    "onlyconfusion = RDDread.filter(lambda line : (\"confus\" in line.lower()))\n",
    "onlyconfusion.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Confusion is the inability to think as clearly or quickly as you normally do.',\n",
       " 'Confusion may come to anyone early or late phase of the life, depending on the reason behind it .',\n",
       " 'Many times, confusion lasts for a very short span and goes away.',\n",
       " 'Confusion is more common in people who are in late stages of the life and often occurs when you have stayed in hospital.',\n",
       " 'Some confused people may have strange or unusual behavior or may act aggressively.',\n",
       " 'A good way to find out if anyone is confused is to question the person their identity i.e. name, age, and the date.',\n",
       " 'If they are little not sure or unable to answer correctly, they are confused']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onlyconfusion.collect() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tutorialspoint.com/apache_spark/apache_spark_core_programming.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputfile = sc.textFile(\"input.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['people are not as beautiful as they look, ',\n",
       " 'as they walk or as they talk.',\n",
       " 'they are only as beautiful  as they love, ',\n",
       " 'as they care as they share.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputfile.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['people',\n",
       " 'are',\n",
       " 'not',\n",
       " 'as',\n",
       " 'beautiful',\n",
       " 'as',\n",
       " 'they',\n",
       " 'look,',\n",
       " '',\n",
       " 'as',\n",
       " 'they',\n",
       " 'walk',\n",
       " 'or',\n",
       " 'as',\n",
       " 'they',\n",
       " 'talk.',\n",
       " 'they',\n",
       " 'are',\n",
       " 'only',\n",
       " 'as',\n",
       " 'beautiful',\n",
       " '',\n",
       " 'as',\n",
       " 'they',\n",
       " 'love,',\n",
       " '',\n",
       " 'as',\n",
       " 'they',\n",
       " 'care',\n",
       " 'as',\n",
       " 'they',\n",
       " 'share.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputfile.flatMap(lambda line: line.split(\" \")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['people', 'are', 'not', 'as', 'beautiful', 'as', 'they', 'look,', ''],\n",
       " ['as', 'they', 'walk', 'or', 'as', 'they', 'talk.'],\n",
       " ['they', 'are', 'only', 'as', 'beautiful', '', 'as', 'they', 'love,', ''],\n",
       " ['as', 'they', 'care', 'as', 'they', 'share.']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputfile.map(lambda line: line.split(\" \")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('people', 1),\n",
       " ('are', 1),\n",
       " ('not', 1),\n",
       " ('as', 1),\n",
       " ('beautiful', 1),\n",
       " ('as', 1),\n",
       " ('they', 1),\n",
       " ('look,', 1),\n",
       " ('', 1),\n",
       " ('as', 1),\n",
       " ('they', 1),\n",
       " ('walk', 1),\n",
       " ('or', 1),\n",
       " ('as', 1),\n",
       " ('they', 1),\n",
       " ('talk.', 1),\n",
       " ('they', 1),\n",
       " ('are', 1),\n",
       " ('only', 1),\n",
       " ('as', 1),\n",
       " ('beautiful', 1),\n",
       " ('', 1),\n",
       " ('as', 1),\n",
       " ('they', 1),\n",
       " ('love,', 1),\n",
       " ('', 1),\n",
       " ('as', 1),\n",
       " ('they', 1),\n",
       " ('care', 1),\n",
       " ('as', 1),\n",
       " ('they', 1),\n",
       " ('share.', 1)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputfile.flatMap(lambda line:line.split(\" \")).map(lambda word: (word, 1)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['people', 'are', 'not', 'as', 'beautiful', 'as', 'they', 'look,', ''], 1),\n",
       " (['as', 'they', 'walk', 'or', 'as', 'they', 'talk.'], 1),\n",
       " (['they', 'are', 'only', 'as', 'beautiful', '', 'as', 'they', 'love,', ''],\n",
       "  1),\n",
       " (['as', 'they', 'care', 'as', 'they', 'share.'], 1)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputfile.map(lambda line:line.split(\" \")).map(lambda word: (word, 1)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('are', 2),\n",
       " ('as', 8),\n",
       " ('', 3),\n",
       " ('walk', 1),\n",
       " ('only', 1),\n",
       " ('share.', 1),\n",
       " ('people', 1),\n",
       " ('not', 1),\n",
       " ('beautiful', 2),\n",
       " ('they', 7),\n",
       " ('look,', 1),\n",
       " ('or', 1),\n",
       " ('talk.', 1),\n",
       " ('love,', 1),\n",
       " ('care', 1)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "counts = inputfile.flatMap(lambda line:line.split(\" \")).map(lambda word: (word, 1)).reduceByKey(lambda x,y: x+y)\n",
    "counts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'(2) PythonRDD[87] at collect at <ipython-input-52-93d337ba8392>:3 []\\n |  MapPartitionsRDD[86] at mapPartitions at PythonRDD.scala:422 []\\n |  ShuffledRDD[85] at partitionBy at NativeMethodAccessorImpl.java:0 []\\n +-(2) PairwiseRDD[84] at reduceByKey at <ipython-input-52-93d337ba8392>:2 []\\n    |  PythonRDD[83] at reduceByKey at <ipython-input-52-93d337ba8392>:2 []\\n    |  input.txt MapPartitionsRDD[26] at textFile at NativeMethodAccessorImpl.java:0 []\\n    |  input.txt HadoopRDD[25] at textFile at NativeMethodAccessorImpl.java:0 []'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts = inputfile.flatMap(line => line.split(\" \")).map(word => (word, 1)).reduceByKey(_+_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
