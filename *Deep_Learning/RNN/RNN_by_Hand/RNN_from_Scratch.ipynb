{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Setup</a></span></li><li><span><a href=\"#Forward-Propagation\" data-toc-modified-id=\"Forward-Propagation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Forward Propagation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Update-State\" data-toc-modified-id=\"Update-State-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Update State</a></span></li><li><span><a href=\"#Forward-States\" data-toc-modified-id=\"Forward-States-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Forward States</a></span></li><li><span><a href=\"#Cost/Loss-Function\" data-toc-modified-id=\"Cost/Loss-Function-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Cost/Loss Function</a></span></li></ul></li><li><span><a href=\"#Backward-Propagation\" data-toc-modified-id=\"Backward-Propagation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Backward Propagation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cost-Gradient\" data-toc-modified-id=\"Cost-Gradient-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Cost Gradient</a></span></li><li><span><a href=\"#Backward\" data-toc-modified-id=\"Backward-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Backward</a></span></li><li><span><a href=\"#gradient-checking\" data-toc-modified-id=\"gradient-checking-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>gradient checking</a></span></li></ul></li><li><span><a href=\"#Optimizer\" data-toc-modified-id=\"Optimizer-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Optimizer</a></span></li><li><span><a href=\"#Testing\" data-toc-modified-id=\"Testing-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Testing</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# RNN From Scrath\n",
    "\n",
    "What better way to learn RNN than to write it from scratch? \n",
    "\n",
    "Goal: Build an RNN that given a sequence of binary numbers, returns the sum of the sequence. For example, given [1,1,0] -> returns 2\n",
    "\n",
    "The original code is from the link below but I made some changes. \n",
    "\n",
    "Credit: http://peterroelants.github.io/posts/rnn_implementation_part01/#Linear-recurrent-neural-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version: 3.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# python version \n",
    "import sys\n",
    "print('python version:', sys.version[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<caption><center> **RNN**  </center></caption>\n",
    "<img src=\"SimpleRNN01.png\" border=\"4\" style=\"width:700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "m = 20 # number of samples \n",
    "sequence_len = 10\n",
    "# Create the sequences\n",
    "X = np.zeros((m, sequence_len))\n",
    "for row_idx in range(m):\n",
    "    X[row_idx,:] = np.around(np.random.rand(sequence_len)).astype(int)\n",
    "# Create the targets for each sequence\n",
    "t = np.sum(X, axis=1) # = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape # 20 examples, 10 sequences each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 1., 0., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 0., 0., 0., 1., 1., 1., 1.],\n",
       "       [0., 0., 1., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 1., 0., 0., 1., 0., 1., 1., 0.],\n",
       "       [1., 0., 0., 1., 1., 0., 1., 1., 1., 1.],\n",
       "       [1., 0., 0., 0., 1., 1., 1., 1., 1., 0.],\n",
       "       [1., 0., 1., 0., 0., 1., 0., 1., 1., 0.],\n",
       "       [1., 1., 1., 0., 1., 0., 0., 1., 0., 0.],\n",
       "       [1., 1., 1., 1., 0., 0., 1., 0., 0., 1.],\n",
       "       [1., 1., 1., 0., 0., 0., 1., 1., 0., 0.],\n",
       "       [0., 1., 0., 1., 1., 0., 1., 0., 1., 0.],\n",
       "       [0., 1., 0., 1., 0., 1., 1., 1., 0., 0.],\n",
       "       [0., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "       [0., 1., 1., 0., 0., 1., 0., 0., 1., 1.],\n",
       "       [1., 1., 1., 0., 1., 1., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 0., 1., 0., 1., 0., 1., 0.],\n",
       "       [0., 1., 1., 1., 0., 1., 0., 1., 1., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 1., 0.],\n",
       "       [1., 0., 1., 1., 0., 1., 1., 1., 0., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print input\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 7., 2., 4., 5., 7., 6., 5., 5., 6., 5., 5., 5., 7., 5., 6., 6.,\n",
       "       7., 2., 7.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print output \n",
    "t # y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Update State\n",
    "\n",
    "Compute activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the forward step functions\n",
    "def update_state(xk, sk, wx, wRec): # (change compute activation)\n",
    "    \"\"\"\n",
    "    Compute activation at time step k (change to t) by using information from precious state(activation) \n",
    "    and current input xk (change to x_t). \n",
    "    Normally, there would be a bais term and tanh activation function. \n",
    "    \n",
    "    k = current time step (change to t)\n",
    "    sk = previous state (change to s_prev or a_prev for previous activation)  \n",
    "    xk = current input (change to xt)   \n",
    "    xw = input weights (w_x)\n",
    "    wRec = recursive weights (w_a) \n",
    "\n",
    "    Returns the activation of current time step t \n",
    "    \"\"\"\n",
    "    \n",
    "    # current activation (a_t) = input * input weight + previous activation * activation weight \n",
    "    # a_t = x_t * w_x + a_prev * x_a\n",
    "    current_state = xk * wx + sk * wRec \n",
    "    \n",
    "    return current_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Forward States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def forward_states(X, wx, wRec): # (change to forward_propagration)\n",
    "    \"\"\"\n",
    "    Unfold the network and compute all state activations given the input X,\n",
    "    and input weights (wx or w_x) and recursive weights (wRec or x_a).\n",
    "    \n",
    "    Return the activations in a matrix, the last column S[:,-1] contains the\n",
    "    final activations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialise the matrix that holds all states for all input sequences.\n",
    "    # The initial state s0 is set to 0.\n",
    "    S = np.zeros((X.shape[0], X.shape[1]+1)) # S.shape: (20, 11) \n",
    "     \n",
    "    # Use the recurrence relation defined by update_state to update the states trough time.\n",
    "    # go through the sequence (10 length)\n",
    "    for k in range(0, X.shape[1]): #(change k to t)\n",
    "        # S[k] = S[k-1] * wRec + X[k] * wx\n",
    "        '''\n",
    "        X[:,k] -> grabs k-th column, meaning grab every example at time k (or t)\n",
    "        S[:,k] -> grabbing the previous state, it will be all 0 for the first loop iteration \n",
    "        '''\n",
    "        S[:,k+1] = update_state(X[:,k], S[:,k], wx, wRec)\n",
    "        \n",
    "    return S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Cost/Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def cost(y_hat, y): \n",
    "    \"\"\"\n",
    "    y = t = target (change to target or y or the true label) \n",
    "    y_hat = output = y_hat (change to y_hat or last activation unit or output)\n",
    "    \n",
    "    Return the MSE between the targets t and the outputs y.\n",
    "    \n",
    "    m = number of samples \n",
    "    \"\"\"\n",
    "    \n",
    "    return ((y - y_hat)**2).sum() / m "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Cost Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def output_gradient(y_hat, y): # change name \n",
    "    \"\"\"\n",
    "    y = t = target (change to target or y or the true label) \n",
    "    y_hat = output = y_hat (change to y_hat or last activation unit or output)\n",
    "    \n",
    "    return -> derivative of MSE = ((t - y)**2).sum() / m\n",
    "    \n",
    "    Compute the gradient of the MSE cost function with respect to the output y_hat or last activation.\n",
    "    Computes the last layer backpropagation \n",
    "    \"\"\"\n",
    "    return 2.0 * (y_hat - y) / m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Backward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def backward_gradient(X, S, grad_out, wRec):\n",
    "    \"\"\"    \n",
    "    Backpropagate the gradient computed at the output (grad_out) through the network.\n",
    "    Accumulate the parameter gradients for wX and wRec by for each layer by addition.\n",
    "    \n",
    "    Return the parameter gradients as a tuple, and the gradients at the output of each layer.\n",
    "    \n",
    "    Formula: \n",
    "        dL/dw_x = 2/m * (a_t - y) * w_a^[] * x_t \n",
    "        dL/dw_a = 2/m * (a_t - y) * w_a^[] * a_(t-1) \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialise the array that stores the gradients of the cost with respect to the states.\n",
    "    grad_over_time = np.zeros((X.shape[0], X.shape[1]+1))\n",
    "    \n",
    "    # the first starts from the last column \n",
    "    grad_over_time[:,-1] = grad_out\n",
    "    \n",
    "    # Set the gradient accumulations to 0 \n",
    "    wx_grad = 0 # w_x_grad \n",
    "    wRec_grad = 0 # w_a_grad \n",
    "    \n",
    "    # loop through each time step (or the length of the sequence)\n",
    "    for k in range(X.shape[1], 0, -1): # X.shape[1] = 10 \n",
    "        \n",
    "        # Compute the parameter gradients and accumulate the results.\n",
    "        wx_grad += np.sum(grad_over_time[:,k] * X[:,k-1])\n",
    "        wRec_grad += np.sum(grad_over_time[:,k] * S[:,k-1])\n",
    "        \n",
    "        # Compute the gradient at the output of the previous layer \n",
    "        grad_over_time[:,k-1] = grad_over_time[:,k] * wRec \n",
    "        \n",
    "    return (wx_grad, wRec_grad), grad_over_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### gradient checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 11)\n",
      "(20,)\n",
      "(-375.6854816395034, -2329.373310054043) (20, 11)\n"
     ]
    }
   ],
   "source": [
    "# Perform gradient checking\n",
    "# Set the weight parameters used during gradient checking\n",
    "params = [1.2, 1.2]  # [wx, wRec]\n",
    "\n",
    "# Set the small change to compute the numerical gradient\n",
    "eps = 1e-7\n",
    "\n",
    "# Forward propagation \n",
    "S = forward_states(X, params[0], params[1])\n",
    "print(S.shape)\n",
    "\n",
    "grad_out = output_gradient(S[:,-1], t) # t = target values  \n",
    "print(grad_out.shape)\n",
    "      \n",
    "backprop_grads, grad_over_time = backward_gradient(X, S, grad_out, params[1]) # why pass in just second param. wRec? \n",
    "print(backprop_grads, grad_over_time.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No gradient errors found\n"
     ]
    }
   ],
   "source": [
    "# Perform gradient checking\n",
    "# Set the weight parameters used during gradient checking\n",
    "params = [1.2, 1.2]  # [wx, wRec]\n",
    "# Set the small change to compute the numerical gradient\n",
    "eps = 1e-7\n",
    "# Compute the backprop gradients\n",
    "S = forward_states(X, params[0], params[1])\n",
    "grad_out = output_gradient(S[:,-1], t)\n",
    "backprop_grads, grad_over_time = backward_gradient(X, S, grad_out, params[1])\n",
    "# Compute the numerical gradient for each parameter in the layer\n",
    "for p_idx, _ in enumerate(params):\n",
    "    grad_backprop = backprop_grads[p_idx]\n",
    "    # + eps\n",
    "    params[p_idx] += eps\n",
    "    plus_cost = cost(forward_states(X, params[0], params[1])[:,-1], t)\n",
    "    # - eps\n",
    "    params[p_idx] -= 2 * eps\n",
    "    min_cost = cost(forward_states(X, params[0], params[1])[:,-1], t)\n",
    "    # reset param value\n",
    "    params[p_idx] += eps\n",
    "    # calculate numerical gradient\n",
    "    grad_num = (plus_cost - min_cost) / (2*eps)\n",
    "    # Raise error if the numerical grade is not close to the backprop gradient\n",
    "    if not np.isclose(grad_num, grad_backprop):\n",
    "        print ('Numerical gradient of {:.6f} is not close to the backpropagation gradient of {:.6f}!'.format(float(grad_num), float(grad_backprop)))\n",
    "print('No gradient errors found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Define Rprop optimisation function\n",
    "def update_rprop(X, t, W, W_prev_sign, W_delta, eta_p, eta_n):\n",
    "    \"\"\"\n",
    "    Update Rprop values in one iteration.\n",
    "    X: input data.\n",
    "    t: targets.\n",
    "    W: Current weight parameters.\n",
    "    W_prev_sign: Previous sign of the W gradient.\n",
    "    W_delta: Rprop update values (Delta).\n",
    "    eta_p, eta_n: Rprop hyperparameters.\n",
    "    \"\"\"\n",
    "    # Perform forward and backward pass to get the gradients\n",
    "    S = forward_states(X, W[0], W[1])\n",
    "    grad_out = output_gradient(S[:,-1], t)\n",
    "    W_grads, _ = backward_gradient(X, S, grad_out, W[1])\n",
    "    W_sign = np.sign(W_grads)  # Sign of new gradient\n",
    "    # Update the Delta (update value) for each weight parameter seperately\n",
    "    for i, _ in enumerate(W):\n",
    "        if W_sign[i] == W_prev_sign[i]:\n",
    "            W_delta[i] *= eta_p\n",
    "        else:\n",
    "            W_delta[i] *= eta_n\n",
    "    return W_delta, W_sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final weights are: wx = 0.9999999999999711,  wRec = 1.0000000000000062\n"
     ]
    }
   ],
   "source": [
    "# Perform Rprop optimisation\n",
    "\n",
    "# Set hyperparameters\n",
    "eta_p = 1.2\n",
    "eta_n = 0.5\n",
    "\n",
    "# Set initial parameters\n",
    "#W = [-1.5, 2]  # [wx, wRec]\n",
    "W = [-10, 7]  # [wx, wRec]\n",
    "W_delta = [0.001, 0.001]  # Update values (Delta) for W\n",
    "W_sign = [0, 0]  # Previous sign of W\n",
    "\n",
    "ls_of_ws = [(W[0], W[1])]  # List of weights to plot\n",
    "# Iterate over 500 iterations\n",
    "for i in range(10000):\n",
    "    # Get the update values and sign of the last gradient\n",
    "    W_delta, W_sign = update_rprop(X, t, W, W_sign, W_delta, eta_p, eta_n)\n",
    "    # Update each weight parameter seperately\n",
    "    for i, _ in enumerate(W):\n",
    "        W[i] -= W_sign[i] * W_delta[i]\n",
    "    ls_of_ws.append((W[0], W[1]))  # Add weights to list to plot\n",
    "\n",
    "print('Final weights are: wx = {0},  wRec = {1}'.format(W[0], W[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target output: 5 vs Model output: 5.00\n"
     ]
    }
   ],
   "source": [
    "test_inpt = np.asmatrix([[0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1]])\n",
    "test_outpt = forward_states(test_inpt, W[0], W[1])[:,-1]\n",
    "print ('Target output: {:d} vs Model output: {:.2f}'.format(test_inpt.sum(), test_outpt[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target output: 7 vs Model output: 7.00\n"
     ]
    }
   ],
   "source": [
    "test_inpt = np.asmatrix([[0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]])\n",
    "test_outpt = forward_states(test_inpt, W[0], W[1])[:,-1]\n",
    "print ('Target output: {:d} vs Model output: {:.2f}'.format(test_inpt.sum(), test_outpt[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target output: 2 vs Model output: 2.00\n"
     ]
    }
   ],
   "source": [
    "test_inpt = np.asmatrix([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]])\n",
    "test_outpt = forward_states(test_inpt, W[0], W[1])[:,-1]\n",
    "print ('Target output: {:d} vs Model output: {:.2f}'.format(test_inpt.sum(), test_outpt[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "177px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
