{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram Frequency Calculator from Scratch\n",
    "\n",
    "Source: https://stackoverflow.com/questions/17531684/n-grams-in-python-four-five-six-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import packages \n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string\n",
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1851\n",
      "                                   MOBY DICK;\n",
      "                                  OR THE WHALE\n",
      "                               by Herman Melville\n",
      "ETYMOLOGY\n",
      "  ETYMOLOGY\n",
      "  (Supplied by a Late Consumptive Usher to a Grammar School)\n",
      "\n",
      "  The pale Usher- threadbare in coat, heart, body, and brain; I see\n",
      "him now. He was ever dusting his old lexicons and grammars, with a\n",
      "queer handkerchief, mockingly embellished with all the gay flags of\n",
      "all the known nations of the world. He loved to dust his old grammars;\n",
      "it somehow mildly reminded him of his mortality.\n",
      "\n",
      "  \"While you take in hand to school others, and to teach them by\n",
      "what name a whale-fish is to be called in our tongue leaving out,\n",
      "through ignorance, the letter H, which almost alone maketh the\n",
      "signification of the word, you deliver that which is not true.\"\n",
      "                                                        HACKLUYT\n",
      "\n",
      "  \"WHALE. * * * Sw. and Dan. hval. This animal is named from roundness\n",
      "or rolling; for in Dan. hvalt is arched or vaulte\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request \n",
    "\n",
    "# download example text as an example \n",
    "url = 'http://www.textfiles.com/etext/FICTION/mobydick'\n",
    "response = urllib.request.urlopen(url) \n",
    "html = response.read()\n",
    "soup = BeautifulSoup(html,\"html5lib\")\n",
    "data = soup.get_text(strip=True)\n",
    "print (data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# since the file is too big, reduce the file \n",
    "data = data[:int(len(data)/3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1851',\n",
       " 'MOBY',\n",
       " 'DICK',\n",
       " 'OR',\n",
       " 'THE',\n",
       " 'WHALE',\n",
       " 'by',\n",
       " 'Herman',\n",
       " 'Melville',\n",
       " 'ETYMOLOGY']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove punctuations \n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "data = data.translate(translator)\n",
    "\n",
    "# split data into tokens \n",
    "tokens = [t for t in data.split()] \n",
    "\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1851',\n",
       " 'MOBY',\n",
       " 'DICK',\n",
       " 'WHALE',\n",
       " 'Herman',\n",
       " 'Melville',\n",
       " 'ETYMOLOGY',\n",
       " 'ETYMOLOGY',\n",
       " 'Supplied',\n",
       " 'Late']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stop words \n",
    "clean_tokens = tokens.copy() # or tokens[:]\n",
    "for token in tokens:\n",
    "    if token.lower() in stopwords.words('english'): \n",
    "        clean_tokens.remove(token)    \n",
    "\n",
    "clean_tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngram(tokens,ngrams): \n",
    "    '''\n",
    "    Looks through each word in the document and grab next ngrams and returns tuple \n",
    "    '''\n",
    "    return [tuple(tokens[i:i+ngrams]) for i in range(len(tokens)-ngrams+1)]\n",
    "\n",
    "def FrequencyCalculator(tokens):\n",
    "    ''' \n",
    "    Given list of tokens, calculate each token's frequency. \n",
    "    By putting the final result into the dictionary, it removes duplicate entries. \n",
    "    '''\n",
    "    word_count = [tokens.count(word) for word in tokens]\n",
    "    return dict(zip(tokens,word_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('one',), 247),\n",
       " (('ye',), 213),\n",
       " (('whale',), 185),\n",
       " (('upon',), 183),\n",
       " (('like',), 175),\n",
       " (('old',), 156),\n",
       " (('man',), 156),\n",
       " (('said',), 153),\n",
       " (('Queequeg',), 152),\n",
       " (('would',), 143),\n",
       " (('Captain',), 122),\n",
       " (('little',), 121),\n",
       " (('Ahab',), 116),\n",
       " (('though',), 114),\n",
       " (('great',), 113),\n",
       " (('sea',), 112),\n",
       " (('ship',), 111),\n",
       " (('yet',), 102),\n",
       " (('seemed',), 101),\n",
       " (('time',), 99)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_words = ngram(clean_tokens,ngrams=1)\n",
    "word_count_dict = FrequencyCalculator(ngram_words)\n",
    "\n",
    "# Show top 20 most frequency words \n",
    "result = sorted(word_count_dict.items(), key=lambda x: x[1],reverse=True)[:20]\n",
    "result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Captain', 'Ahab'), 41),\n",
       " (('Captain', 'Peleg'), 30),\n",
       " (('Moby', 'Dick'), 25),\n",
       " (('Sperm', 'Whale'), 22),\n",
       " (('white', 'whale'), 19),\n",
       " (('New', 'Bedford'), 16),\n",
       " (('old', 'man'), 13),\n",
       " (('sperm', 'whale'), 13),\n",
       " (('young', 'man'), 13),\n",
       " (('Captain', 'Bildad'), 13),\n",
       " (('Mrs', 'Hussey'), 12),\n",
       " (('Aye', 'aye'), 12),\n",
       " (('Cape', 'Horn'), 11),\n",
       " (('one', 'side'), 11),\n",
       " (('one', 'hand'), 10),\n",
       " (('ye', 'ye'), 10),\n",
       " (('never', 'mind'), 9),\n",
       " (('something', 'like'), 9),\n",
       " (('dont', 'know'), 9),\n",
       " (('Father', 'Mapple'), 9)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_words2 = ngram(clean_tokens,ngrams=2)\n",
    "word_count_dict2 = FrequencyCalculator(ngram_words2)\n",
    "\n",
    "# Show top 20 most frequency words \n",
    "result2 = sorted(word_count_dict2.items(), key=lambda x: x[1],reverse=True)[:20]\n",
    "result2 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
